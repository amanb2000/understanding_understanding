{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import umap\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_activations(internal_reps, layer_idx):\n",
    "    \"\"\"\n",
    "    Get the activations of a specific layer for all problems and all examples.\n",
    "\n",
    "    internal_reps: list[tuple[tensor]] where internal_reps[question_num][layer_num]\n",
    "     is a tensor of shape [batch=1, num_tokens, d_model=4096]\n",
    "\n",
    "    We will return a tensor of shape [num_questions, num_tokens, d_model=4096]\n",
    "    for the given layer\n",
    "    \"\"\"\n",
    "    return torch.cat([internal_reps[i][layer_idx] for i in range(len(internal_reps))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## x + y UMAP 20240709\n",
    "\n",
    "```bash\n",
    "# 100 instances, 10 possible answers 15-20\n",
    "python3 compute_minute_math_reps.py \\\n",
    "    --output_dir results/xyUMAP20240709 \\\n",
    "    --problem_type xy\n",
    "\n",
    "# 10,000 instances, 10 possible answers 15-20\n",
    "python3 compute_minute_math_reps.py \\\n",
    "    --output_dir results/xy10kUMAP20240709 \\\n",
    "    --problem_type xy \\\n",
    "    --num_unique_problems 10000\n",
    "\n",
    "# 1,000 instances\n",
    "python3 compute_minute_math_reps.py \\\n",
    "    --output_dir results/xy1kUMAP20240709 \\\n",
    "    --problem_type xy \\\n",
    "    --num_unique_problems 1000\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the results\n",
    "results_dir = 'xyUMAP20240709_cam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers shape: (100, 1)\n",
      "Problems shape: (100, 17)\n",
      "Internal Reps type: <class 'list'>\n",
      "Internal Reps length: 100\n",
      "Logits type: <class 'torch.Tensor'>\n",
      "Logits length: 100\n",
      "Example internal representation shape: torch.Size([17, 4096])\n",
      "Example logits shape: torch.Size([17, 128256])\n",
      "\n",
      "Example answers: [868]\n",
      "\n",
      "Example problems: [128000     87    284    220     22     11    379    284    220     23\n",
      "     26   9093    865    489    379    284    220]\n",
      "\n",
      "Example internal representation: tensor([[-8.2970e-05,  2.5749e-04, -2.4605e-04,  ..., -3.2425e-04,\n",
      "         -2.1553e-04,  4.7112e-04],\n",
      "        [-2.0752e-03, -1.4038e-03,  6.1035e-03,  ...,  2.2888e-03,\n",
      "          6.1951e-03,  1.1414e-02],\n",
      "        [ 1.0376e-03, -6.8054e-03,  6.2943e-04,  ...,  2.5482e-03,\n",
      "         -8.3618e-03, -8.8501e-03],\n",
      "        ...,\n",
      "        [ 3.3569e-03, -3.3760e-04,  2.4719e-03,  ..., -8.3008e-03,\n",
      "          3.2654e-03, -7.3242e-03],\n",
      "        [ 1.0376e-03, -6.8054e-03,  6.2943e-04,  ...,  2.5482e-03,\n",
      "         -8.3618e-03, -8.8501e-03],\n",
      "        [-6.3324e-04,  1.0395e-04,  1.6327e-03,  ..., -1.9684e-03,\n",
      "          7.7057e-04, -1.6479e-03]], device='cuda:0')\n",
      "\n",
      "Example logits: tensor([[ 4.8914,  6.0422, 10.7782,  ..., -3.6068, -3.6067, -3.6066],\n",
      "        [ 5.0844,  5.5250,  5.3040,  ..., -4.7326, -4.7326, -4.7328],\n",
      "        [ 3.0377,  2.2906, -1.2401,  ..., -0.5274, -0.5273, -0.5272],\n",
      "        ...,\n",
      "        [ 7.1848,  5.5690,  4.4517,  ..., -2.1464, -2.1458, -2.1460],\n",
      "        [ 0.2240,  2.4152,  3.9246,  ...,  0.2736,  0.2743,  0.2742],\n",
      "        [ 2.5393, -1.3449,  3.9985,  ..., -3.3005, -3.3003, -3.3004]])\n"
     ]
    }
   ],
   "source": [
    "# Load the answers and problems files\n",
    "answers = np.load(os.path.join(results_dir, 'answers.npy'))\n",
    "problems = np.load(os.path.join(results_dir, 'problems.npy'))\n",
    "\n",
    "# Load the internal representations and logits\n",
    "internal_reps = torch.load(os.path.join(results_dir, 'internal_reps.pt'))\n",
    "logits = torch.load(os.path.join(results_dir, 'logits.pt'))\n",
    "\n",
    "# Load the arguments\n",
    "with open(os.path.join(results_dir, 'args.json'), 'r') as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "# Display the shapes and types of the loaded data\n",
    "print(\"Answers shape:\", answers.shape)\n",
    "print(\"Problems shape:\", problems.shape)\n",
    "print(\"Internal Reps type:\", type(internal_reps))\n",
    "print(\"Internal Reps length:\", len(internal_reps))\n",
    "print(\"Logits type:\", type(logits))\n",
    "print(\"Logits length:\", len(logits))\n",
    "\n",
    "# Let's inspect the internal representations and logits a bit more closely\n",
    "print(\"Example internal representation shape:\", internal_reps[0][0][0].shape)\n",
    "print(\"Example logits shape:\", logits[0].shape)\n",
    "\n",
    "# Display some examples from the loaded data\n",
    "print(\"\\nExample answers:\", answers[0])\n",
    "print(\"\\nExample problems:\", problems[0])\n",
    "print(\"\\nExample internal representation:\", internal_reps[0][0][0])\n",
    "print(\"\\nExample logits:\", logits[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correctness of model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# compute the actual token-wise answers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args['model_name'])\n",
    "\n",
    "# find the argmax over the logits for each \n",
    "# problem to get the predicted class\n",
    "\n",
    "# logits has shape [num_questions, num_tokens, vocab_size]\n",
    "\n",
    "final_logits = logits[:, -1, :]\n",
    "\n",
    "# take the argmax over the final dim\n",
    "predicted_class = torch.argmax(final_logits, dim=1)\n",
    "\n",
    "# decode each prediction individually\n",
    "predicted_class_str = [tokenizer.decode(i) for i in predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_answers = [tokenizer.decode(i) for i in answers[:, 0]]\n",
    "\n",
    "str_problems = []\n",
    "for i in range(problems.shape[0]):\n",
    "    str_problems.append(tokenizer.decode(problems[i, :]).replace('\\n', ' '))\n",
    "\n",
    "num_correct = 0\n",
    "for i in range(len(str_answers)):\n",
    "    if str_answers[i] == predicted_class_str[i]:\n",
    "        num_correct += 1\n",
    "\n",
    "num_correct / len(str_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>x = 7, y = 8; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 11, y = 5; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 3, y = 14; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 12, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 15, y = 4; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 14, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 2, y = 19; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 20, y = 2; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 0, y = 23; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 17, y = 7; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 1, y = 24; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 9, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 8, y = 8; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 0, y = 17; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 10, y = 8; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 6, y = 13; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 7, y = 13; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 6, y = 15; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 2, y = 20; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 17, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 23, y = 1; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 0, y = 25; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 1, y = 14; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 13, y = 3; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 15, y = 2; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 8, y = 10; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 2, y = 17; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 16, y = 4; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 12, y = 9; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 6, y = 16; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 20, y = 3; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 12, y = 12; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 11, y = 14; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 7, y = 8; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 5, y = 11; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 8, y = 9; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 5, y = 13; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 8, y = 11; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 17, y = 3; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 6, y = 15; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 15, y = 7; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 22, y = 1; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 2, y = 22; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 21, y = 4; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 11, y = 4; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 12, y = 4; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 4, y = 13; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 9, y = 9; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 9, y = 10; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 18, y = 2; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 10, y = 11; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 13, y = 9; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 17, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 21, y = 3; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 6, y = 19; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 9, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 7, y = 9; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 12, y = 5; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 5, y = 13; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 0, y = 19; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 7, y = 13; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 7, y = 14; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 10, y = 12; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 8, y = 15; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 18, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 9, y = 16; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 14, y = 1; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 11, y = 5; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 8, y = 9; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 13, y = 5; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 13, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 13, y = 7; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 15, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 22, y = 0; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 18, y = 5; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 5, y = 19; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 24, y = 1; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 11, y = 4; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 10, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 16, y = 1; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 9, y = 9; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 3, y = 16; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 17, y = 3; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 11, y = 10; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 15, y = 7; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 6, y = 17; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 14, y = 10; therefore x + y = ',\n",
       " '<|begin_of_text|>x = -1, y = 26; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 9, y = 6; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 8, y = 8; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 3, y = 14; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 7, y = 11; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 6, y = 13; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 13, y = 7; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 1, y = 20; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 5, y = 17; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 23, y = 0; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 22, y = 2; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 25, y = 0; therefore x + y = ',\n",
       " '<|begin_of_text|>x = 14, y = 1; therefore x + y = ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a set of integer-valued answers\n",
    "int_answers = [int(a) for a in str_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ids = torch.tensor(answers[:, 0])\n",
    "class_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_umap(layer_reps, class_ids):\n",
    "    \"\"\" Compute the UMAP of layer_reps, which is a tensor of shape [num_examples, num_tokens, token_dim]\n",
    "    \"\"\"\n",
    "    print(\"Layer reps shape:\", layer_reps.shape)\n",
    "    print(\"Layer reps has type:\", type(layer_reps))\n",
    "\n",
    "    class_ids = class_ids.cpu()\n",
    "    if type(layer_reps) != np.ndarray: \n",
    "        layer_reps = layer_reps.cpu()\n",
    "    \n",
    "    # Convert to numpy if it's a torch tensor\n",
    "    if torch.is_tensor(layer_reps):\n",
    "        layer_reps = layer_reps.detach().cpu().numpy()\n",
    "    \n",
    "    # Flatten the last two dimensions\n",
    "    try:\n",
    "        num_examples, num_tokens, token_dim = layer_reps.shape\n",
    "        flattened_reps = layer_reps.reshape(num_examples, num_tokens * token_dim)\n",
    "    except: \n",
    "        assert len(layer_reps.shape) == 2, \"Layer reps shape is messed up\"\n",
    "        flattened_reps = layer_reps\n",
    "\n",
    "    \n",
    "    # Apply UMAP\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    umap_embedding = reducer.fit_transform(flattened_reps)\n",
    "    # print(\"Output of reduce transform: \", umap_embedding)\n",
    "    \n",
    "    return umap_embedding\n",
    "\n",
    "def get_class_relevance_pca(layer_reps, class_ids):\n",
    "    \"\"\" Compute the PCA of layer_reps, which is a tensor of shape [num_examples, num_tokens, token_dim]\n",
    "    We are going to compute the PCA which maximizes the variance of class means\n",
    "    \"\"\"\n",
    "    flattened_reps = layer_reps.reshape(layer_reps.shape[0], -1).cpu()\n",
    "    \n",
    "    # Calculate class means efficiently\n",
    "    unique_classes = np.unique(class_ids)\n",
    "    print(\"flattened_reps type: \", type(flattened_reps))\n",
    "    print(\"flattened_reps device: \", flattened_reps.device)\n",
    "    print(\"class_ids type: \", type(class_ids))\n",
    "    print(\"class_ids device: \", class_ids.device)\n",
    "    class_means = np.array([flattened_reps[class_ids == c].mean(axis=0) for c in unique_classes])\n",
    "    \n",
    "    # Run PCA on the class means\n",
    "    pca = PCA()\n",
    "    pca.fit(class_means)\n",
    "    \n",
    "    # Project the original data onto the PCA components\n",
    "    projected_data = pca.transform(flattened_reps)\n",
    "    \n",
    "    return projected_data\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_pca(layer_reps, class_ids):\n",
    "    \"\"\" Compute the PCA of layer_reps, which is a tensor of shape [num_examples, num_tokens, token_dim]\n",
    "    \"\"\"\n",
    "    # Convert to numpy if it's a torch tensor\n",
    "    if torch.is_tensor(layer_reps):\n",
    "        layer_reps = layer_reps.detach().cpu().numpy()\n",
    "    \n",
    "    # Flatten the last two dimensions\n",
    "    num_examples, num_tokens, token_dim = layer_reps.shape\n",
    "    flattened_reps = layer_reps.reshape(num_examples, num_tokens * token_dim)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)  # You can adjust the number of components as needed\n",
    "    pca_result = pca.fit_transform(flattened_reps)\n",
    "    \n",
    "    return pca_result\n",
    "\n",
    "def get_class_pca_and_umap(layer_reps, class_ids):\n",
    "    \"\"\" Compute the PCA and UMAP of layer_reps, which is a tensor of shape [num_examples, num_tokens, token_dim]\n",
    "    \"\"\"\n",
    "    pca_result = get_class_relevance_pca(layer_reps, class_ids)\n",
    "\n",
    "    number_of_class_ids = len(np.unique(class_ids)) - 1\n",
    "\n",
    "    pca_and_umap_result = get_umap(pca_result[:, 0:number_of_class_ids], class_ids)\n",
    "    \n",
    "    return pca_and_umap_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_reps type:  <class 'torch.Tensor'>\n",
      "flattened_reps device:  cpu\n",
      "class_ids type:  <class 'torch.Tensor'>\n",
      "class_ids device:  cpu\n",
      "Layer reps shape: (100, 10)\n",
      "Layer reps has type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/understanding_understanding/venv/lib/python3.10/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# for dim_reduction_func in (get_pca, get_umap, get_class_relevance_pca, get_class_pca_and_umap):\n",
    "for dim_reduction_func in (get_class_pca_and_umap,):\n",
    "    # iterate through each layer\n",
    "    num_layers = 33\n",
    "    purity_by_layer = []\n",
    "    for layer_num in range(num_layers):\n",
    "        layer_activations = get_layer_activations(internal_reps, layer_num)\n",
    "        low_dim_embedding = dim_reduction_func(layer_activations, class_ids)\n",
    "        \n",
    "        # Create Plotly figure\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "        \n",
    "        # Add scatter plot\n",
    "        scatter = go.Scatter(\n",
    "            x=low_dim_embedding[:, 0],\n",
    "            y=low_dim_embedding[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=int_answers,\n",
    "                colorscale='Viridis',\n",
    "                showscale=True\n",
    "            ),\n",
    "            text=[f\"Problem: {prob}<br>Answer: {ans}<br>Predicted: {pred}\" \n",
    "                for prob, ans, pred in zip(str_problems, str_answers, predicted_class_str)],\n",
    "            hoverinfo='text'\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(scatter)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'{dim_reduction_func.__name__} for Layer {layer_num}',\n",
    "            xaxis_title='Dim 1',\n",
    "            yaxis_title='Dim 2',\n",
    "            width=1000,\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        # Save as interactive HTML\n",
    "        pio.write_html(fig, file=f'{results_dir}/{dim_reduction_func.__name__}_layer_{layer_num}.html')\n",
    "        \n",
    "        # Save as static PNG\n",
    "        pio.write_image(fig, file=f'{results_dir}/{dim_reduction_func.__name__}_layer_{layer_num}.png')\n",
    "        \n",
    "        # Clear the figure to free up memory\n",
    "        fig.data = []\n",
    "        fig.layout = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate through each layer\n",
    "# num_layers = 33\n",
    "# purity_by_layer = []\n",
    "# for layer_num in range(num_layers):\n",
    "#     layer_activations = get_layer_activations(internal_reps, layer_num)\n",
    "#     umap_embedding = get_umap(layer_activations)\n",
    "    \n",
    "#     # Plot UMAP\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     scatter = plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=int_answers, cmap='viridis')\n",
    "#     plt.colorbar(scatter)\n",
    "#     plt.title(f'UMAP for Layer {layer_num}')\n",
    "#     plt.savefig(f'{results_dir}/umap_layer_{layer_num}.png')\n",
    "#     plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.36s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.1851, -0.2059, -1.8382,  ..., -2.8908,  1.3605,  0.3109],\n",
       "         [ 1.2942, -4.6553,  0.2416,  ...,  2.4082,  0.4292, -1.4888],\n",
       "         [ 1.6030, -0.9961,  2.2922,  ...,  1.3186, -0.8656, -1.0692],\n",
       "         ...,\n",
       "         [-0.6426,  2.8010,  4.5676,  ..., -0.4665,  0.7432, -2.6349],\n",
       "         [-1.4301,  0.0569,  2.6330,  ...,  1.0594,  0.2765, -3.0647],\n",
       "         [-2.4644, -3.2000,  1.7012,  ..., -1.0187,  1.5276,  1.0285]],\n",
       "\n",
       "        [[ 4.1851, -0.2059, -1.8382,  ..., -2.8908,  1.3605,  0.3109],\n",
       "         [ 1.2942, -4.6553,  0.2416,  ...,  2.4082,  0.4292, -1.4888],\n",
       "         [ 1.6030, -0.9961,  2.2922,  ...,  1.3186, -0.8656, -1.0692],\n",
       "         ...,\n",
       "         [-1.1591,  3.2138,  3.5799,  ..., -0.2368,  0.2975, -3.6477],\n",
       "         [-1.3349, -0.3476,  2.1352,  ...,  1.3411,  0.0254, -4.7679],\n",
       "         [-0.4112, -3.9738,  3.0329,  ...,  0.5366,  2.0905, -0.1498]],\n",
       "\n",
       "        [[ 4.1851, -0.2059, -1.8382,  ..., -2.8908,  1.3605,  0.3109],\n",
       "         [ 1.2942, -4.6553,  0.2416,  ...,  2.4082,  0.4292, -1.4888],\n",
       "         [ 1.6030, -0.9961,  2.2922,  ...,  1.3186, -0.8656, -1.0692],\n",
       "         ...,\n",
       "         [-0.9592,  3.0631,  3.3667,  ..., -0.3133,  0.4255, -3.6954],\n",
       "         [-0.4207,  0.5844,  1.9329,  ...,  0.5118,  0.5327, -3.8135],\n",
       "         [ 0.9663, -0.6553,  1.2528,  ..., -1.3672,  2.7668,  1.5143]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.1851, -0.2059, -1.8382,  ..., -2.8908,  1.3605,  0.3109],\n",
       "         [ 1.2942, -4.6553,  0.2416,  ...,  2.4082,  0.4292, -1.4888],\n",
       "         [ 1.6030, -0.9961,  2.2922,  ...,  1.3186, -0.8656, -1.0692],\n",
       "         ...,\n",
       "         [-1.1389,  2.1765,  3.4575,  ..., -0.4641,  0.7297, -3.7126],\n",
       "         [-2.2051, -0.7697,  2.0608,  ...,  1.0523,  0.3850, -3.7949],\n",
       "         [-4.0609, -2.2500,  1.5351,  ..., -0.5199,  1.2357,  0.5011]],\n",
       "\n",
       "        [[ 4.1851, -0.2059, -1.8382,  ..., -2.8908,  1.3605,  0.3109],\n",
       "         [ 1.2942, -4.6553,  0.2416,  ...,  2.4082,  0.4292, -1.4888],\n",
       "         [ 1.6030, -0.9961,  2.2922,  ...,  1.3186, -0.8656, -1.0692],\n",
       "         ...,\n",
       "         [-1.4083,  2.9817,  3.6705,  ..., -0.4616,  0.6264, -3.4454],\n",
       "         [-1.2836,  0.3655,  1.7992,  ...,  0.5618, -0.5308, -3.5866],\n",
       "         [-1.1146, -3.8207,  0.3295,  ..., -1.5954,  1.6417,  1.6805]],\n",
       "\n",
       "        [[ 4.1851, -0.2059, -1.8382,  ..., -2.8908,  1.3605,  0.3109],\n",
       "         [ 1.2942, -4.6553,  0.2416,  ...,  2.4082,  0.4292, -1.4888],\n",
       "         [ 1.6030, -0.9961,  2.2922,  ...,  1.3186, -0.8656, -1.0692],\n",
       "         ...,\n",
       "         [-1.0249,  2.5733,  3.3920,  ..., -0.3490,  0.5552, -3.7669],\n",
       "         [-0.2017, -0.2848,  2.1559,  ...,  0.5293, -0.1084, -3.9482],\n",
       "         [-1.1388, -4.1318,  1.8962,  ..., -3.1782,  1.5691,  0.8038]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_activations = model.model.norm(layer_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 17, 4096])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_activations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing if the last hidden state gives the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids shape:  torch.Size([1, 15])\n",
      "Logits shape:  torch.Size([1, 15, 128256])\n",
      "Hidden states shapes:  [torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096]), torch.Size([1, 15, 4096])]\n",
      "Last hidden state shape:  torch.Size([1, 15, 4096])\n"
     ]
    }
   ],
   "source": [
    "input_string= \"Hello, my name is Albert Einstein, and I am a physicist.\"\n",
    "input_ids = tokenizer(input_string, return_tensors='pt')['input_ids'].to(model.device)\n",
    "\n",
    "# run thru model, grab logits and hidden states and last_hidden_states\n",
    "outputs = model(input_ids, output_hidden_states=True, return_dict=True)\n",
    "logits = outputs.logits\n",
    "hidden_states = outputs.hidden_states\n",
    "last_hidden_state = hidden_states[-1]\n",
    "\n",
    "# print the shapes of each \n",
    "print(\"Input ids shape: \", input_ids.shape)\n",
    "print(\"Logits shape: \", logits.shape)\n",
    "print(\"Hidden states shapes: \", [hs.shape for hs in hidden_states])\n",
    "print(\"Last hidden state shape: \", last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_logits = model.lm_head(last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 128256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 128256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if logits are all close\n",
    "torch.allclose(logits, approx_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state - hidden_states[-1]\n",
    "# check if all close\n",
    "torch.allclose(last_hidden_state, hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx2_logits = model.lm_head(hidden_states[-1])\n",
    "# check if all close\n",
    "torch.allclose(logits, approx2_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Lipschitz Ratios for Each Layer')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each layer of the model, we are going to take the embedding of the 0th element (flattened)\n",
    "# Then, we are going to compute its distance from the 10th, 20th, 30th, etc. elements\n",
    "# We will then plot this distance as a function of the layer number\n",
    "# Plot all of this on a single plot, where each line is labelled by \"x + y = z\", but instead of x and y, it will be the corresponding values from str_problems\n",
    "num_layers = 33\n",
    "layer_activations_0 = get_layer_activations(internal_reps, 0)\n",
    "num_examples, num_tokens, token_dim = layer_activations_0.shape\n",
    "distances = torch.zeros((num_layers, num_examples, num_examples))\n",
    "for layer_num in range(num_layers):\n",
    "        layer_activations = get_layer_activations(internal_reps, layer_num)\n",
    "\n",
    "        # Flatten the last two dimensions\n",
    "        num_examples, num_tokens, token_dim = layer_activations.shape\n",
    "        flattened_reps = layer_activations.reshape(num_examples, num_tokens * token_dim)\n",
    "\n",
    "        # normalize flattened_reps\n",
    "        # flattened_reps = F.normalize(flattened_reps, p=2, dim=1)\n",
    "\n",
    "        # Compute the pairwise distance between each pair of examples\n",
    "        distances[layer_num] = (flattened_reps[:, None, :] - flattened_reps[None, :, :]).norm(p=2, dim=-1)\n",
    "        \n",
    "# Compute Lipschitz ratios for each layer_i -> layer_i+1\n",
    "# Compute Lipschitz ratios for each layer_i -> layer_i+1\n",
    "lipschitz_ratio_mean = []\n",
    "lipschitz_ration_std = []\n",
    "for i in range(num_layers - 1):\n",
    "    dist_ratio = distances[i+1] / (distances[i]+1e-6)\n",
    "    dist_ratio[torch.diag(torch.ones(num_examples)).bool()] = 0\n",
    "    lipschitz_ratio_mean += [torch.mean(dist_ratio).item()]\n",
    "    lipschitz_ration_std += [torch.std(dist_ratio).item()]\n",
    "    print(torch.sum(torch.tensor(lipschitz_ratio_mean)< 1.0))\n",
    "\n",
    "# Plot the Lipschitz ratio mean, then plot error as shaded region at 1, 2 standard deviations.\n",
    "plt.errorbar(range(1, num_layers), lipschitz_ratio_mean, yerr=[lipschitz_ration_std, lipschitz_ration_std], fmt='o')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Lipschitz Ratio')\n",
    "plt.title('Lipschitz Ratios for Each Layer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
